{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a38cf08",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "#from tensorflow.python.keras.utils import CustomObjectScope\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Input, Reshape, Add\n",
    "from keras.utils.generic_utils import get_custom_objects\n",
    "from keras.regularizers import l1_l2, l2, l1\n",
    "from keras.metrics import categorical_accuracy\n",
    "from tensorflow.keras.backend import eval\n",
    "\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "from scipy.optimize import nnls\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import keras\n",
    "import glob\n",
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "import math\n",
    "import time\n",
    "import random\n",
    "import socket\n",
    "\n",
    "\n",
    "import innvestigate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c17b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_dir = 'fuzzgoat' #Make this customizable for the future\n",
    "\n",
    "IP = \"127.0.0.1\"\n",
    "PORT = 4455\n",
    "ADDR = (IP, PORT)\n",
    "SIZE = 1024\n",
    "FORMAT = \"iso-8859-1\"\n",
    "\n",
    "checkpoint_path = './targets/' + target_dir + '/inc_learn/model.hd5'\n",
    "\n",
    "seed_list = glob.glob('./targets/' + target_dir + '/out/queue/id*') #Note that you will need seperate input and output directories between the two times the fuzzer is run\n",
    "new_seeds = glob.glob('./targets/' + target_dir + '/out/queue/id*') #Will be relevent for incremental learning, no need for it currently\n",
    "print('done')\n",
    "\n",
    "argvv = sys.argv[1:] #This is supposed to give you the name of the program you are running (.exe)\n",
    "call = subprocess.check_output\n",
    "print(sys.argv)\n",
    "\n",
    "os.path.isdir(\"./targets/\" + target_dir + \"/bitmaps/\") or os.makedirs(\"./targets/\" + target_dir + \"/bitmaps/\")\n",
    "os.path.isdir(\"./targets/\" + target_dir + \"/in/gen/\") or os.makedirs(\"./targets/\" + target_dir + \"/in/gen/\")\n",
    "os.path.isdir(\"./targets/\" + target_dir + \"/checkpoint\") or os.makedirs(\"./targets/\" + target_dir + \"/checkpoint\")\n",
    "os.path.isdir(\"./targets/\" + target_dir + \"/out/queue_grads\") or os.makedirs(\"./targets/\" + target_dir + \"/out/queue_grads\")\n",
    "os.path.isdir(\"./targets/\" + target_dir + \"/inc_learn\") or os.makedirs(\"./targets/\" + target_dir + \"/inc_learn\")\n",
    "\n",
    "print(seed_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a09e0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sys.argv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b846d2d",
   "metadata": {},
   "source": [
    "Setting up bitmaps and getting data (Courtesy of Neuzz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11479012",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocessing():\n",
    "    global MAX_BITMAP_SIZE\n",
    "    global MAX_FILE_SIZE\n",
    "    global seed_list\n",
    "    global new_seeds\n",
    "    global counter\n",
    "    global label\n",
    "    global u_idx\n",
    "    \n",
    "    \n",
    "    seed_list = glob.glob('./targets/' + target_dir + '/out/queue/id*')\n",
    "    seed_list.sort()\n",
    "    SPLIT_RATIO = len(seed_list)\n",
    "    rand_index = np.arange(SPLIT_RATIO)\n",
    "    np.random.shuffle(seed_list)\n",
    "    new_seeds = glob.glob('./targets/' + target_dir + '/out/queue/id*') #Once again will be relevent later for incremental learning\n",
    "    \n",
    "    #cwd = os.getcwd()\n",
    "    \n",
    "    max_file_name = call(['ls', '-S', './targets/' + target_dir + '/out/queue/']).decode('utf8').split('\\n')[0].rstrip('\\n')\n",
    "    MAX_FILE_SIZE = os.path.getsize('./targets/' + target_dir + '/out/queue/' + max_file_name)\n",
    "    #print(seed_list)\n",
    "    \n",
    "        # obtain raw bitmaps\n",
    "    raw_bitmap = {}\n",
    "    tmp_cnt = []\n",
    "    out = ''\n",
    "    for file in seed_list:\n",
    "        tmp_list = []\n",
    "        try:\n",
    "            # append \"-o tmp_file\" to strip's arguments to avoid tampering tested binary.\n",
    "            if argvv[0] == './strip':\n",
    "                out = call(['./afl-showmap', '-q', '-e', '-o', '/dev/stdout', '-m', '512', '-t', '500'] + ['./targets/' + target_dir + '/' + target_dir] + [file] + ['-o', 'tmp_file'])\n",
    "                #out = call(['./afl-showmap', '-q', '-e', '-o', '/dev/stdout', '-m', '512', '-t', '500'] + argvv + [file] + ['-o', 'tmp_file'])\n",
    "            else:\n",
    "                out = call(['./afl-showmap', '-q', '-e', '-o', '/dev/stdout', '-m', '512', '-t', '500'] + ['./targets/' + target_dir + '/' + target_dir] + [file])\n",
    "                #print(out)\n",
    "                #out = call(['./afl-showmap', '-q', '-e', '-o', '/dev/stdout', '-m', '512', '-t', '500'] + argvv + [file])\n",
    "        except subprocess.CalledProcessError:\n",
    "            print(\"find a crash\")\n",
    "        #print(out)\n",
    "        #print(\"end\")\n",
    "        for line in out.splitlines():\n",
    "            edge = line.split(b':')[0]\n",
    "            #print(edge)\n",
    "            tmp_cnt.append(edge)\n",
    "            tmp_list.append(edge)\n",
    "        raw_bitmap[file] = tmp_list\n",
    "        #print(raw_bitmap)\n",
    "    counter = Counter(tmp_cnt).most_common()\n",
    "    #print(\"Counter:\")\n",
    "    #print(counter)\n",
    "    #print(raw_bitmap)\n",
    "\n",
    "    # save bitmaps to individual numpy label\n",
    "    label = [int(f[0]) for f in counter]\n",
    "    #print(label)\n",
    "    bitmap = np.zeros((len(seed_list), len(label)))\n",
    "    for idx, i in enumerate(seed_list):\n",
    "        #print(idx)\n",
    "        tmp = raw_bitmap[i]\n",
    "        for j in tmp:\n",
    "            #print(j)\n",
    "            if int(j) in label:\n",
    "                bitmap[idx][label.index((int(j)))] = 1\n",
    "\n",
    "    # label dimension reduction\n",
    "    fit_bitmap, u_idx = np.unique(bitmap, axis=1, return_index=True)\n",
    "    #print(\"data dimension\" + str(fit_bitmap.shape))\n",
    "    #print(\"data dimension\" + str(bitmap.shape))\n",
    "\n",
    "    #print(fit_bitmap)    \n",
    "    #print(bitmap)\n",
    "    # save training data\n",
    "    MAX_BITMAP_SIZE = fit_bitmap.shape[1]\n",
    "    for idx, i in enumerate(seed_list):\n",
    "        file_name = \"./targets/\" + target_dir + \"/bitmaps/\" + i.split('/')[-1]\n",
    "        np.save(file_name, fit_bitmap[idx])\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b13a1d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessing()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db153b49",
   "metadata": {},
   "source": [
    "Accuracy Metric (Jaccard Similarity Index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f99a7921",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute jaccard accuracy for multiple label\n",
    "def jaccard(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float64)\n",
    "    y_pred = tf.cast(y_pred, tf.float64)\n",
    "    y_true = tf.round(y_true)\n",
    "    y_pred = tf.round(y_pred)\n",
    "    matching = tf.cast(tf.math.count_nonzero(tf.equal(y_true,y_pred)), tf.float64)\n",
    "    denom = tf.cast(tf.math.add(tf.size(y_true), tf.size(y_pred)), tf.float64)\n",
    "    denom2 = tf.cast(tf.math.subtract(denom, matching), tf.float64)\n",
    "    return tf.cast(tf.divide(matching, denom2), tf.float64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c7c804e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare input into usable vectors (dehex-ifying  the file and storying it as a numerical array)\n",
    "def vectorize(f):\n",
    "    seed = np.zeros((1, MAX_FILE_SIZE))\n",
    "    tmp = open(f, 'rb').read()\n",
    "    ln = len(tmp)\n",
    "    if ln < MAX_FILE_SIZE:\n",
    "        tmp = tmp + (MAX_FILE_SIZE - ln) * b'\\x00'#adding empty bytes to fill up smaller files\n",
    "    seed[0] = [j for j in bytearray(tmp)]\n",
    "    seed = seed.astype('float64')\n",
    "    np.set_printoptions(threshold=sys.maxsize)\n",
    "    print(seed)\n",
    "    seed = seed / 255 #normalize with 255 because maximum hex value is FF = 255\n",
    "    print(seed)\n",
    "    \n",
    "    #print(tmp)\n",
    "    \n",
    "    #loading in bitmap\n",
    "    file_name = \"./targets/\" + target_dir + \"/bitmaps/\" + f.split('/')[-1] + \".npy\"\n",
    "    bitmap = np.load(file_name)\n",
    "    return seed, bitmap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3973c05b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prepare input into usable vectors (dehex-ifying  the file and storying it as a numerical array)\n",
    "def vectorize_range(l, h):\n",
    "    \n",
    "    seeds = np.zeros((h - l, MAX_FILE_SIZE))\n",
    "    bitmaps = np.zeros((h - l, MAX_BITMAP_SIZE))\n",
    "    \n",
    "    for i in range(l, h):\n",
    "        seed, bitmap = vectorize(seed_list[i])\n",
    "        seeds[i-l] = seed\n",
    "        bitmaps[i-l] = bitmap\n",
    "    \n",
    "    return seeds, bitmaps\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dea58df",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorize(seed_list[11])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cc6181",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data():\n",
    "    X, y = [[],[]]\n",
    "    for i in seed_list:\n",
    "        tmp = vectorize(i)\n",
    "        X.append([tmp[0]])\n",
    "        y.append([tmp[1]])\n",
    "        \n",
    "    print(len(X))\n",
    "    print(len(y))\n",
    "    return train_test_split(X, y, test_size=0.2, random_state=1) #Want to preserve this split in case for live updates\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e887853",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateSchedulerPerBatch(LearningRateScheduler):\n",
    "    \"\"\" Callback class to modify the default learning rate scheduler to operate each batch\"\"\"\n",
    "    def __init__(self, schedule, verbose=0):\n",
    "        super(LearningRateSchedulerPerBatch, self).__init__(schedule, verbose)\n",
    "        self.count = 0  # Global batch index (the regular batch argument refers to the batch index within the epoch)\n",
    "\n",
    "    def on_epoch_begin(self, epoch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        pass\n",
    "\n",
    "    def on_batch_begin(self, batch, logs=None):\n",
    "        super(LearningRateSchedulerPerBatch, self).on_epoch_begin(self.count, logs)\n",
    "\n",
    "    def on_batch_end(self, batch, logs=None):\n",
    "        super(LearningRateSchedulerPerBatch, self).on_epoch_end(self.count, logs)\n",
    "        self.count += 1\n",
    "        \n",
    "        \n",
    "class ModelCheckpointEnhanced(ModelCheckpoint):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        # Added arguments\n",
    "        self.callbacks_to_save = kwargs.pop('callbacks_to_save')\n",
    "        self.callbacks_filepath = kwargs.pop('callbacks_filepath')\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        # Run normal flow:\n",
    "        super().on_epoch_end(epoch,logs)\n",
    "\n",
    "        # If a checkpoint was saved, save also the callback\n",
    "        filepath = self.callbacks_filepath.format(epoch=epoch + 1, **logs)\n",
    "        if self.epochs_since_last_save == 0 and epoch!=0:\n",
    "            if self.save_best_only:\n",
    "                current = logs.get(self.monitor)\n",
    "                if current == self.best:\n",
    "                    # Note, there might be some cases where the last statement will save on unwanted epochs.\n",
    "                    # However, in the usual case where your monitoring value space is continuous this is not likely\n",
    "                    with open(filepath, \"wb\") as f:\n",
    "                        pickle.dump(self.callbacks_to_save, f)\n",
    "            else:\n",
    "                with open(filepath, \"wb\") as f:\n",
    "                    pickle.dump(self.callbacks_to_save, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "058e7f7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_model():\n",
    "    batch_size = 32\n",
    "    num_classes = MAX_BITMAP_SIZE\n",
    "    epochs = 50\n",
    "\n",
    "    model = Sequential()\n",
    "    model.add(Dense(1000, input_dim=MAX_FILE_SIZE))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(Dense(num_classes))\n",
    "    model.add(Activation('sigmoid'))\n",
    "\n",
    "    #opt = tf.keras.optimizers.adam(lr=0.0001)\n",
    "\n",
    "    model.compile(loss='binary_crossentropy', optimizer='adam', metrics=[jaccard])\n",
    "    model.summary()\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb709698",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model):\n",
    "    X_train, X_val, y_train, y_val = prepare_data()\n",
    "    X_train = np.array(X_train).reshape(-1, MAX_FILE_SIZE)\n",
    "    y_train = np.array(y_train).reshape(-1, MAX_BITMAP_SIZE)\n",
    "    X_val = np.array(X_val).reshape(-1, MAX_FILE_SIZE)\n",
    "    y_val = np.array(y_val).reshape(-1, MAX_BITMAP_SIZE)\n",
    "    \n",
    "    print(X_train.shape)\n",
    "    print(MAX_FILE_SIZE)\n",
    "    \n",
    "   # ckpt_callback = ModelCheckpoint(filepath='./targets/' + target_dir + '/inc_learn/model.h5', monitor='val_loss')\n",
    "   # lr_decay_callback = LearningRateSchedulerPerBatch(\n",
    "   #                lambda step: ((learning_rate - min_learning_rate) * decay_rate ** step + min_learning_rate))\n",
    "\n",
    "    #callbacks_list = [ckpt_callback, lr_decay_callback]\n",
    "    print(X_train.shape)\n",
    "    #Option insert learning decay rate here\n",
    "    his = model.fit(X_train, y_train, validation_data = (X_val, y_val),  epochs = 50, batch_size=32) #treating the entire X-train as one input rn?????\n",
    "    # Save model and weights\n",
    "    model.save_weights(\"./targets/\" + target_dir + \"model.h5\")\n",
    "    return model, his"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c132a43",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62dc9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def step_decay(epoch):\n",
    "    initial_lrate = 0.001\n",
    "    drop = 0.7\n",
    "    epochs_drop = 10.0\n",
    "    lrate = initial_lrate * math.pow(drop, math.floor((1 + epoch) / epochs_drop))\n",
    "    return lrate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cbbe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.002\n",
    "min_learning_rate = 0.0001\n",
    "decay_rate = 0.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9483bbf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one(model):\n",
    "    X_train, X_val, y_train, y_val = prepare_data()\n",
    "    X_train = np.array(X_train).reshape(-1, MAX_FILE_SIZE)\n",
    "    y_train = np.array(y_train).reshape(-1, MAX_BITMAP_SIZE)\n",
    "    X_val = np.array(X_val).reshape(-1, MAX_FILE_SIZE)\n",
    "    y_val = np.array(y_val).reshape(-1, MAX_BITMAP_SIZE)\n",
    "    \n",
    "    print(X_train.shape)\n",
    "    print(MAX_FILE_SIZE)\n",
    "    \n",
    "    ckpt_callback = ModelCheckpoint(filepath='./targets/' + target_dir + '/inc_learn/model.h5', monitor='val_loss')\n",
    "    lr_decay_callback = LearningRateSchedulerPerBatch(\n",
    "                    lambda step: ((learning_rate - min_learning_rate) * decay_rate ** step + min_learning_rate))\n",
    "\n",
    "    callbacks_list = [ckpt_callback, lr_decay_callback]\n",
    "    print(X_train.shape)\n",
    "    #Option insert learning decay rate here\n",
    "    for i in range(len(X_train)):\n",
    "        his = model.fit(X_train[i].reshape(1, MAX_FILE_SIZE), y_train[i].reshape(1, MAX_BITMAP_SIZE), validation_data = (X_val, y_val),  epochs = 50, callbacks=callbacks_list, batch_size=32) #treating the entire X-train as one input rn?????\n",
    "    # Save model and weights\n",
    "    model.save_weights(\"./targets/\" + target_dir + \"model.h5\")\n",
    "    return model, his"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08e0b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    \n",
    "    model = gen_model()\n",
    "    his = train(model)\n",
    "    \n",
    "    return model, his"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e3dbc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model, his = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec11d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc_model(x, y):\n",
    "    # Initialize Callbacks:\n",
    "    lr_decay_callback = LearningRateSchedulerPerBatch(\n",
    "                        lambda step: ((learning_rate - min_learning_rate) * decay_rate ** step + min_learning_rate))\n",
    "    # Modified Checkpoint Callback\n",
    "    ckpt_callback = ModelCheckpointEnhanced(filepath='./targets/' + target_dir + '/inc_learn/model.h5', monitor='val_loss',\n",
    "                                    callbacks_to_save=lr_decay_callback ,callbacks_filepath='./checkpoint/LRcallback.{epoch:02d}.pickle' )\n",
    "\n",
    "    callbacks = [ckpt_callback, lr_decay_callback]\n",
    "\n",
    "    # Load checkpoint:\n",
    "    if checkpoint_path is not None:\n",
    "        # Load model:\n",
    "        model = load_model(checkpoint_path, custom_objects={\"jaccard\": jaccard})\n",
    "        # Finding the epoch index from which we are resuming\n",
    "        initial_epoch = get_init_epoch(checkpoint_path)\n",
    "        # loading the callback from pickle\n",
    "        loaded_callback = pickle.load(open( callback_checkpoint_path, \"rb\" ))\n",
    "        # Update the callback instance\n",
    "        callbacks[1] = loaded_callback\n",
    "    else:\n",
    "        model = gen_model()\n",
    "        initial_epoch = 0\n",
    "    # Start/resume training\n",
    "    model.fit(x, y, callbacks=callbacks, initial_epoch=initial_epoch)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa0f550",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = prepare_data()\n",
    "X_train = np.array(X_train).reshape(-1, MAX_FILE_SIZE)\n",
    "y_train = np.array(y_train).reshape(-1, MAX_BITMAP_SIZE)\n",
    "X_val = np.array(X_val).reshape(-1, MAX_FILE_SIZE)\n",
    "y_val = np.array(y_val).reshape(-1, MAX_BITMAP_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc8e500",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_val[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c93d65",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "f = plt.figure(figsize=(20,8))\n",
    "\n",
    "#accuracy\n",
    "plt1 = f.add_subplot(121)\n",
    "plt1.plot(his.history['jaccard'], label = str('Training accuracy'))\n",
    "plt1.plot(his.history['val_jaccard'], label = str('Validation accuracy'))\n",
    "plt.legend()\n",
    "plt.title('accuracy')\n",
    "\n",
    "#loss\n",
    "plt2 = f.add_subplot(122)\n",
    "plt2.plot(his.history['loss'], label = str('Training loss'))\n",
    "plt2.plot(his.history['val_loss'], label = str('Validation loss'))\n",
    "plt.legend()\n",
    "plt.title('loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b424301",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_generate(batch_size):\n",
    "    global seed_list\n",
    "    while 1:\n",
    "        np.random.shuffle(seed_list)\n",
    "        # load a batch of training data\n",
    "        for i in range(0, len(seed_list), batch_size):\n",
    "            # load full batch\n",
    "            if (i + batch_size) > len(seed_list):\n",
    "                x, y = vectorize_range(i, len(seed_list))\n",
    "                x = x.astype('float32') / 255\n",
    "            # load remaining data for last batch\n",
    "            else:\n",
    "                x, y = vectorize_range(i, i + batch_size)\n",
    "                x = x.astype('float32') / 255\n",
    "            yield (x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9294a81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inc_model():\n",
    "    loss_history = LossHistory()\n",
    "    lrate = keras.callbacks.LearningRateScheduler(step_decay)\n",
    "    callbacks_list = [loss_history, lrate]\n",
    "    model.fit_generator(train_generate(16),\n",
    "                        steps_per_epoch=(len(seed_list) / 16 + 1),\n",
    "                        epochs=50,\n",
    "                        verbose=1, callbacks=callbacks_list)\n",
    "    # Save model and weights\n",
    "    model.save_weights(\"hard_label.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacfa352",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "inc_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb79a72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_gan(verbose = False):\n",
    "    disc_model = dist_model()\n",
    "    train(disc_model)\n",
    "    \n",
    "    gen_model = gen_model()\n",
    "    \n",
    "    gan = Sequential()\n",
    "    gan.add(gen_model)\n",
    "    gan.add(disc_model)\n",
    "    \n",
    "    disc_model.trainable = False\n",
    "    \n",
    "    gan.compile(loss='binary_crossentropy', optimizer='adam', metrics=[jaccard])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "697bf0a3",
   "metadata": {},
   "source": [
    "Idea: Run the intial neural network in reverse to generate desired edge coverage patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e13946ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#model2 = tf.keras.Model(inputs=model.input, outputs=model.layers[-2].output)\n",
    "\n",
    "#Reversing the prebuilt model\n",
    "sample_map = np.array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1., 1., 1.,\n",
    "        1., 1., 1., 1., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
    "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 1., 1., 1., 1., 1., 1.,\n",
    "        1., 1., 1., 1., 1., 1., 1., 1.])\n",
    "input_map = sample_map\n",
    "\n",
    "\n",
    "\n",
    "for i in range(len(input_map)):\n",
    "    if input_map[i] == 0:\n",
    "        input_map[i] += 1e-5\n",
    "    if input_map[i] == 1:\n",
    "        input_map[i] -= 1e-5\n",
    "       \n",
    "#input_map = np.array(input_map)\n",
    "#print(input_map)\n",
    "#Working through inverting sigmoid acitvation\n",
    "inv_sig = tf.subtract(1.0, input_map)\n",
    "inv_sig = tf.divide(input_map, inv_sig)\n",
    "inv_sig = tf.math.log(inv_sig)\n",
    "inv_sig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9644546",
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_5 = inv_sig.eval(session=tf.compat.v1.Session())\n",
    "print(dense_5)\n",
    "dense_5 -= model.layers[2].get_weights()[1]\n",
    "weights5 = model.layers[2].get_weights()[0]\n",
    "weights5 = weights5.transpose()\n",
    "print(weights5)\n",
    "activation_4 = np.linalg.lstsq(weights5, dense_5, rcond=None)[0]\n",
    "#activation_4 = nnls(weights5, dense_5)[0]\n",
    "#skipping RELU Activation\n",
    "dense_3 = activation_4\n",
    "dense_3 -= model.layers[0].get_weights()[1]\n",
    "weights3 = model.layers[0].get_weights()[0]\n",
    "weights3 = weights3.transpose()\n",
    "#gen_input = np.linalg.lstsq(weights3, dense_3, rcond=None)[0]\n",
    "gen_input = nnls(weights3, dense_3)[0]\n",
    "\n",
    "with np.printoptions(threshold = np.inf):\n",
    "    print(gen_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30615f95",
   "metadata": {},
   "outputs": [],
   "source": [
    "check = np.rint(model(gen_input.reshape(-1, MAX_FILE_SIZE)))\n",
    "print(check)\n",
    "print(np.rint(sample_map))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51027f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking result\n",
    "layer_output = model.layers[1].output\n",
    "inter_model = tf.keras.models.Model(inputs=model.input, outputs=layer_output)\n",
    "inter_pred = inter_model(X_train[0].reshape(-1, MAX_FILE_SIZE))\n",
    "inter_pred\n",
    "#X_train.shape\n",
    "#MAX_BITMAP_SIZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f5b8b0",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Computing the Gradient\n",
    "analyzer = innvestigate.create_analyzer(\"integrated_gradients\", model, neuron_selection_mode=\"index\")\n",
    "a = analyzer.analyze(X_train)\n",
    "\n",
    "#with np.printoptions(threshold = np.inf):\n",
    "    #print(a[0])\n",
    "    #print(X_train[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce5b0a6",
   "metadata": {},
   "source": [
    "Ok for real now:\n",
    "The plan is simple\n",
    "Gradient guided mutation but based on paths that have appeared with the least frequency\n",
    "Get gradient of input to specific output neuron that is covered little from the counter\n",
    "then do mutation magic\n",
    "\n",
    "If least frequent appears more than X times, then proceed to flip largest gradient overall\n",
    "If least frequent counter doesnt change in y interations, flip largest gradient overall too"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa154bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSmallestPath():\n",
    "    \n",
    "    path = counter[-1]\n",
    "    idx = label.index(int(path[0]))\n",
    "    \n",
    "    try:\n",
    "        found = list(u_idx).index(idx)\n",
    "    except ValueError:\n",
    "        found = -1\n",
    "        \n",
    "    i = -2\n",
    "    while(found == -1):\n",
    "        path = counter[i]\n",
    "        idx = label.index(int(path[0]))\n",
    "\n",
    "        try:\n",
    "            found = list(u_idx).index(idx)\n",
    "        except ValueError:\n",
    "            found = -1\n",
    "\n",
    "        #Figure out an alternative method for here because .index crashes the program if not found\n",
    "        i -= 1\n",
    "        \n",
    "    return found\n",
    "\n",
    "\n",
    "def getLargestPath(): #Idea: Instead of trying to turn on the least visited path, avoid the most visited paths\n",
    "    \n",
    "    path = counter[0]\n",
    "    idx = label.index(int(path[0]))\n",
    "    try:\n",
    "        found = list(u_idx).index(idx)\n",
    "    except ValueError:\n",
    "        found = -1\n",
    "    \n",
    "    i = 1\n",
    "    while(found == -1):\n",
    "        path = counter[i]\n",
    "        idx = label.index(int(path[0]))\n",
    "        try:\n",
    "            found = list(u_idx).index(idx)\n",
    "        except ValueError:\n",
    "            found = -1\n",
    "        \n",
    "        #Figure out an alternative method for here because .index crashes the program if not found\n",
    "        i += 1\n",
    "        \n",
    "    return found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd6c53d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSmallestGradient(curr_seed):\n",
    "    analyzer = innvestigate.create_analyzer(\"integrated_gradients\", model, neuron_selection_mode=\"index\")\n",
    "    \n",
    "    target_index = getSmallestPath()\n",
    "    \n",
    "    a = analyzer.analyze(curr_seed, target_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e54b9bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing the Gradient\n",
    "analyzer = innvestigate.create_analyzer(\"integrated_gradients\", model, neuron_selection_mode=\"index\")\n",
    "\n",
    "#Determining the index of the least frequented bit\n",
    "\n",
    "#If an index cant be found (because it was removed by np.unique) just look at the next smallest item in the counter,\n",
    "#this always works because if the index is dependant on another one, we will always stumble into the right because its count\n",
    "#is the same. Otherwise if a new path is found then the bitmap gets updated and the previously missing index will appear\n",
    "\n",
    "#target_index = getSmallestPath()\n",
    "\n",
    "path = counter[18]\n",
    "idx = label.index(int(path[0]))\n",
    "target_index = list(u_idx).index(idx) #Figure out an alternative method for here because .index crashes the program if not found\n",
    "#print(idx)\n",
    "#print(u_idx) #Technically u_idx should be sorted in reverse order already anyways\n",
    "    \n",
    "\n",
    "#print(X_val[0]) #Replace with current fuzzing input\n",
    "curr_in = np.expand_dims(X_val[0], axis=0)\n",
    "\n",
    "a = analyzer.analyze(curr_in, target_index) #If weights are too small either mutate with overall gradient or add new inputs\n",
    "\n",
    "with np.printoptions(threshold = np.inf):\n",
    "    print(a)\n",
    "    print(target_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4816465e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computing the Gradient\n",
    "analyzer = innvestigate.create_analyzer(\"integrated_gradients\", model)\n",
    "\n",
    "#Determining the index of the least frequented bit\n",
    "\n",
    "#If an index cant be found (because it was removed by np.unique) just look at the next smallest item in the counter,\n",
    "#this always works because if the index is dependant on another one, we will always stumble into the right because its count\n",
    "#is the same. Otherwise if a new path is found then the bitmap gets updated and the previously missing index will appear\n",
    "\n",
    "target_index = getSmallestPath()\n",
    "\n",
    "#print(X_val[0]) #Replace with current fuzzing input\n",
    "curr_in = np.expand_dims(X_val[2], axis=0)\n",
    "print(curr_in.shape)\n",
    "print(MAX_FILE_SIZE)\n",
    "\n",
    "\n",
    "a = analyzer.analyze(curr_in)\n",
    "\n",
    "with np.printoptions(threshold = np.inf):\n",
    "    print(a)\n",
    "    print(target_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c51cf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mut_one():\n",
    "    '''\n",
    "    Takes a series of inputs that were used to be fuzzed, calculates the overall gradient of each set of inputs and generates\n",
    "    a new set of inputs for each that essentially flips inputs with the largest gradient\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "180d65e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mut_assist():\n",
    "    '''\n",
    "    Idea for this mutator is to just send gradient information over to AFL and instead adjust the various fuzzing strategies\n",
    "    within afl as a way to fuzz.\n",
    "    \n",
    "    i.e. chanings the perf_score in calculate score or changing bit_flip priorities\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf990b4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getSmallestPaths(fn, start, num_paths): #Changed input_vec to fn so that we can get the bitmap\n",
    "    cnt = 0\n",
    "    num_found = 0\n",
    "    idxs = []\n",
    "    path = counter[-1]\n",
    "    #print(path)\n",
    "    #print(u_idx)\n",
    "    idx = label.index(int(path[0]))\n",
    "    #print(idx)\n",
    "    \n",
    "    file_name = \"./targets/\" + target_dir + \"/bitmaps/\" + fn.split('/')[-1] + \".npy\"\n",
    "    bitmap = np.load(file_name)\n",
    "    \n",
    "    try:\n",
    "        found = list(u_idx).index(idx)\n",
    "        if(bitmap[found] == 0 and start < cnt):\n",
    "            idxs.append(found)\n",
    "            cnt += 1\n",
    "\n",
    "    except ValueError:\n",
    "        pass\n",
    "        \n",
    "    i = -2\n",
    "    while(cnt < num_paths):\n",
    "        #print(i)\n",
    "        path = counter[i]\n",
    "        idx = label.index(int(path[0]))\n",
    "        try:\n",
    "            found = list(u_idx).index(idx)\n",
    "            if found != -1 and bitmap[found] == 0 and start <= num_found:\n",
    "                idxs.append(found)\n",
    "                cnt += 1\n",
    "            if found != -1:\n",
    "                num_found += 1\n",
    "\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        #Figure out an alternative method for here because .index crashes the program if not found\n",
    "        i -= 1\n",
    "        \n",
    "        if i <= -len(counter) or cnt >= num_paths:\n",
    "            break\n",
    "        #Add condition to break if i becomes greater than the size of the counter before num_paths is fufilled        \n",
    "    return idxs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a311128",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_on = getSmallestPaths(seed_list[0], 50, 55)\n",
    "#idx_on = tf.convert_to_tensor(idx_on)\n",
    "idx_on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c65bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b333c4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_categorical_accuracy(y_true, y_pred):\n",
    "    #y_pred = tf.gather(y_pred, indices=idx_on)\n",
    "    #y_true = tf.gather(y_true, indices=idx_on)\n",
    "    \n",
    "    return K.cast(K.equal(K.argmax(y_true, axis=-1),\n",
    "                          K.argmax(y_pred, axis=-1)),\n",
    "                  K.floatx())\n",
    "\n",
    "# compute jaccard accuracy for multiple label\n",
    "def adv_jaccard(y_true, y_pred):\n",
    "    y_true = tf.cast(y_true, tf.float64)\n",
    "    y_pred = tf.cast(y_pred, tf.float64)\n",
    "    y_true = tf.reshape(tf.round(y_true), [-1])\n",
    "    y_pred = tf.reshape(tf.round(y_pred), [-1])\n",
    "    \n",
    "    y_pred = tf.gather(y_pred, indices=idx_on)\n",
    "    y_true = tf.gather(y_true, indices=idx_on)\n",
    "    \n",
    "    matching = tf.cast(tf.math.count_nonzero(tf.equal(y_true,y_pred)), tf.float64)\n",
    "    #print(matching)\n",
    "    denom = tf.cast(tf.math.add(tf.size(y_true), tf.size(y_pred)), tf.float64)\n",
    "    #print(\"mean\")\n",
    "    denom2 = tf.cast(tf.math.subtract(denom, matching), tf.float64)\n",
    "    #print(K.mean(tf.cast(tf.divide(matching, denom2), tf.float64)))\n",
    "    return tf.cast(tf.divide(matching, denom2), tf.float64)  \n",
    "\n",
    "def hamming_loss(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    XOR TT for reference -\n",
    "     \n",
    "    A  B   Output\n",
    "     \n",
    "    0  0    0\n",
    "    0  1    1\n",
    "    1  0    1\n",
    "    1  1    0\n",
    "    \"\"\"\n",
    "    hl_num = np.sum(np.logical_xor(y_true, y_pred))\n",
    "    hl_den = np.prod(y_true.shape)\n",
    "     \n",
    "    return hl_num/hl_den\n",
    "\n",
    "def alpha_evaluation_score(y_true, y_pred):\n",
    "    alpha = 1\n",
    "    beta = 0.25\n",
    "    gamma = 1\n",
    "     \n",
    "    # compute true positives across training examples and labels\n",
    "    tp = tf.add(tf.logical_and(y_true, y_pred))\n",
    "     \n",
    "    # compute false negatives (Missed Labels) across training examples and labels\n",
    "    fn = tf.add(tf.logical_and(y_true, tf.logical_not(y_pred)))\n",
    "     \n",
    "    # compute False Positive across training examples and labels.\n",
    "    fp = tf.add(tf.logical_and(tf.logical_not(y_true), y_pred))\n",
    "         \n",
    "    # Compute alpha evaluation score\n",
    "    alpha_score = (1 - ((beta * fn + gamma * fp ) / (tp +fn + fp + 0.00001)))**alpha\n",
    "    \n",
    "    return alpha_score\n",
    "     \n",
    "    \n",
    "def adv_BinaryCrossEntropy(y_true, y_pred): \n",
    "    y_true = tf.reshape(y_true, [-1])\n",
    "    y_pred = tf.reshape(y_pred, [-1])\n",
    "    y_pred = tf.gather(y_pred, indices=idx_on)\n",
    "    y_true = tf.gather(y_true, indices=idx_on)\n",
    "    \n",
    "    y_pred = K.clip(y_pred, K.epsilon(), 1 - K.epsilon())\n",
    "    term_0 = (1 - y_true) * K.log(1 - y_pred + K.epsilon())  \n",
    "    term_1 = y_true * K.log(y_pred + K.epsilon())\n",
    "    return -K.mean(term_0 + term_1, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e38d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#function for generating an adversarial example given a base p_input, adversarial class target, classifier, and regularization type\n",
    "def generate_adversary(p_in,target_vector,model,regularization,loss_function):\n",
    "    \n",
    "    #input for base p_input\n",
    "    p_input = Input(shape=(1, MAX_FILE_SIZE),name='p_input')#Consider trying to reshape each of these to its transpose (MAX_FILE_SIZE, 1)\n",
    "\n",
    "    #unit input for adversarial noise\n",
    "    one = Input(shape=(1,), name='unity')\n",
    "    print(\"ran\")\n",
    "    #layer for learning adversarial noise to apply to p_input\n",
    "    noise = Dense(MAX_FILE_SIZE,activation = None,use_bias=False,kernel_initializer='random_normal',\n",
    "                  kernel_regularizer=regularization, name='adversarial_noise')(one)\n",
    "    \n",
    "    #reshape noise in shape of p_input\n",
    "    noise = Reshape((1, MAX_FILE_SIZE),name='reshape')(noise)\n",
    "    \n",
    "    #add noise to p_input\n",
    "    net = Add(name='add')([noise,p_input])\n",
    "    #clip values to be within 0.0 and 1.0\n",
    "    net = Activation('clip', name='clip_values')(net)\n",
    "    \n",
    "    #feed adversarial p_input to trained model\n",
    "    outputs = model(net)\n",
    "\n",
    "    adversarial_model = Model(inputs=[p_input,one], outputs=outputs)\n",
    "    #freeze trained MNIST classifier layers\n",
    "    adversarial_model.layers[-1].trainable = False\n",
    "    print(\"ran2\")\n",
    "    adversarial_model.compile(optimizer='adam', loss=loss_function, metrics=[adv_jaccard]) #Categorical is most likely not great for this purpose\n",
    "\n",
    "    #callback for saving weights with smallest loss\n",
    "    checkpoint = ModelCheckpoint('./targets/' + target_dir + '/checkpoint/adversarial_weights.h5', monitor='loss', verbose=0, save_best_only=True, save_weights_only=True,\n",
    "                                 mode='auto', period=1)\n",
    "    print(\"ran3\")\n",
    "    print(adversarial_model.summary())\n",
    "    #train adversarial p_input\n",
    "    adversarial_model.fit(x=[p_in.reshape(-1, 1, MAX_FILE_SIZE),np.ones(shape=(1,1))],y=target_vector.reshape(-1, 1, MAX_BITMAP_SIZE),epochs=5000,verbose=1, callbacks=[checkpoint])\n",
    "    print(\"ran4\")\n",
    "    #restore best weights\n",
    "    adversarial_model.load_weights('./targets/' + target_dir + '/checkpoint/adversarial_weights.h5')\n",
    "    \n",
    "    #quantize adversarial noise\n",
    "    quantized_weights = np.round(adversarial_model.get_weights()[0].reshape((1, MAX_FILE_SIZE)) * 255.) / 255.\n",
    "    \n",
    "    #add trained weights to original p_input and clip values to produce adversarial p_input\n",
    "    adversarial_p_in = np.clip(p_in.reshape((1,MAX_FILE_SIZE)) + quantized_weights, 0., 1.)\n",
    "    \n",
    "    #display adversarial p_input\n",
    "    #print(p_in)\n",
    "    #print(adversarial_p_in) #check what kind of p_input is fed into the network\n",
    "    #plt.imshow(adversarial_p_in,vmin=0., vmax=1.)\n",
    "    #plt.show()\n",
    "    #classify adversarial p_input\n",
    "    #adversarial_prediction = fmodel.predict(adversarial_p_in.reshape((1,28,28,1)))\n",
    "    #print(adversarial_prediction)\n",
    "    \n",
    "    return adversarial_p_in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1f43f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#custom activation function for keeping adversarial values between 0.0 and 1.0\n",
    "def clip(x):\n",
    "    return K.clip(x, 0.0, 1.0)\n",
    "\n",
    "\n",
    "def mut_adv(fn):\n",
    "    '''\n",
    "    Generates an aversarial example that targets less visited nodes and feeds it as input for the fuzzer to use as a seed\n",
    "    '''\n",
    "    #First Create a Target Vector of Values\n",
    "    path = counter[0]\n",
    "    idx = label.index(int(path[0]))\n",
    "    zeros = []\n",
    "    ones = []\n",
    "    try:\n",
    "        zeros.append(list(u_idx).index(idx))\n",
    "    except ValueError:\n",
    "        pass\n",
    "    \n",
    "    i = 1\n",
    "    #First half of paths that are most frequented are set to 0 and second half are set to 1\n",
    "    while(i <= len(counter)/2): #Thought to perhaps disclude nodes from items if execution path appears in every single generated case\n",
    "        path = counter[i]\n",
    "        idx = label.index(int(path[0]))\n",
    "        try:\n",
    "            zeros.append(list(u_idx).index(idx))\n",
    "        except ValueError:\n",
    "            pass\n",
    "            \n",
    "        #Figure out an alternative method for here because .index crashes the program if not found\n",
    "        i += 1\n",
    "\n",
    "    while(i < len(counter)):\n",
    "        path = counter[i]\n",
    "        idx = label.index(int(path[0]))\n",
    "        try:\n",
    "            ones.append(list(u_idx).index(idx))\n",
    "        except ValueError:\n",
    "            pass\n",
    "\n",
    "        i += 1\n",
    "        \n",
    "    \n",
    "    target_vector = np.zeros(MAX_BITMAP_SIZE)\n",
    "    target_idx = getSmallestPaths(fn, 0, 10)\n",
    "    \n",
    "    for i in target_idx:\n",
    "        target_vector[i] = 1\n",
    "\n",
    "    #add custom objects to dictionary\n",
    "    get_custom_objects().update({'clip': clip})\n",
    "    #print(target_vector)\n",
    "    #print(target_vector.reshape(1, -1))\n",
    "\n",
    "\n",
    "    p_in = np.random.normal(.5, .3, (1, MAX_FILE_SIZE))\n",
    "    p_in,bit = vectorize(fn)\n",
    "\n",
    "    \n",
    "    adv_jaccard(model.predict(p_in).flatten(), target_vector)\n",
    "    \n",
    "    #Instead of using a random vector pick one from a previous set that maximizes the gradient?\n",
    "    \n",
    "    print(p_in)\n",
    "    #generate_adversary(img,9,model,l1(0.01),'categorical_crossentropy')\n",
    "    \n",
    "   # with CustomObjectScope({'clip': Activation(clip)}):\n",
    "    adv = generate_adversary(p_in,target_vector,model,l2(0.001),adv_BinaryCrossEntropy) #greater the value the more subtle the noise\n",
    "    #pred(generate_adversary(img,9,model,l1_l2(l1=0.01,l2=0.01),'categorical_crossentropy')\n",
    "    \n",
    "    return adv, target_vector\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519db0c5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    \n",
    "#loss_func = tf.keras.losses.CosineSimilarity\n",
    "\n",
    "adv_in, target_vector = mut_adv(seed_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c207d3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(adv_in)\n",
    "pred = model.predict(adv_in)\n",
    "with np.printoptions(threshold = np.inf):\n",
    "    r_pred = np.rint(pred).flatten()\n",
    "    for i in idx_on:\n",
    "        print(r_pred[i])\n",
    "        \n",
    "    print(r_pred)\n",
    "    print(target_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37fe59f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "    \n",
    "#loss_func = tf.keras.losses.CosineSimilarity\n",
    "\n",
    "adv_in, target_vector = mut_adv(seed_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0024efd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.trainable = False\n",
    "\n",
    "    \n",
    "target_vector = np.zeros(MAX_BITMAP_SIZE)\n",
    "target_idx = getSmallestPaths(fn, 0, 10)\n",
    "\n",
    "for i in target_idx:\n",
    "    target_vector[i] = 1\n",
    "\n",
    "\n",
    "loss_object = adv_BinaryCrossEntropy#tf.keras.losses.Categorical_Crossentropy()\n",
    "def create_adversarial_pattern(input_image, input_label):\n",
    "  with tf.GradientTape() as tape:\n",
    "    tape.watch(input_image)\n",
    "\n",
    "    #forward propagate image and retrieve the loss\n",
    "    prediction = pretrained_model(input_image)\n",
    "    loss = loss_object(input_label, prediction)\n",
    "\n",
    "   # obtain the gradient of the loss with respect to the input image\n",
    "   gradient = tape.gradient(loss, input_image)\n",
    "\n",
    "   # Get the sign of the gradients (directions)\n",
    "   signed_grad = tf.sign(gradient)\n",
    "   return signed_grad\n",
    "\n",
    "epsilons = [0, 0.01, 0.1, 0.15]\n",
    "descriptions = [('Epsilon = {:0.3f}'.format(eps) if eps else 'Input')\n",
    "                for eps in epsilons]\n",
    "for i, eps in enumerate(epsilons):\n",
    "  adv_x = image - eps*perturbations #input_image - epsilon * gradients\n",
    "  adv_x = tf.clip_by_value(adv_x, -1, 1)\n",
    "  adv_jaccard(model.predict(adv_x), )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb619874",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(idx_on)\n",
    "def adv_jaccard(y_pred, y_true):\n",
    "    y_true = tf.cast(y_true, tf.float64)\n",
    "    y_pred = tf.cast(y_pred, tf.float64)\n",
    "    y_true = tf.reshape(tf.round(y_true), [-1])\n",
    "    y_pred = tf.reshape(tf.round(y_pred), [-1])\n",
    "    \n",
    "    print(tf.keras.backend.get_value(y_pred))\n",
    "    print(tf.keras.backend.get_value(y_true))\n",
    "    \n",
    "    y_pred = tf.gather(y_pred, indices=idx_on)\n",
    "    y_true = tf.gather(y_true, indices=idx_on)\n",
    "    \n",
    "    print(tf.keras.backend.get_value(y_pred))\n",
    "    print(tf.keras.backend.get_value(y_true))\n",
    "    \n",
    "    matching = tf.cast(tf.math.count_nonzero(tf.equal(y_true,y_pred)), tf.float64)\n",
    "    #print(matching)\n",
    "    denom = tf.cast(tf.math.add(tf.size(y_true), tf.size(y_pred)), tf.float64)\n",
    "    #print(\"mean\")\n",
    "    denom2 = tf.cast(tf.math.subtract(denom, matching), tf.float64)\n",
    "    #print(K.mean(tf.cast(tf.divide(matching, denom2), tf.float64)))\n",
    "    return tf.cast(tf.divide(matching, denom2), tf.float64)  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faec397",
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vector = np.zeros(MAX_BITMAP_SIZE)\n",
    "target_idx = getSmallestPaths(seed_list[0], 0, 5)\n",
    "print(target_idx)\n",
    "\n",
    "for i in target_idx:\n",
    "    target_vector[i] = 1\n",
    "\n",
    "#add custom objects to dictionary\n",
    "get_custom_objects().update({'clip': clip})\n",
    "#print(target_vector)\n",
    "#print(target_vector.reshape(1, -1))\n",
    "\n",
    "\n",
    "p_in = np.random.normal(.5, .3, (1, MAX_FILE_SIZE))\n",
    "p_in,bit = vectorize(seed_list[0])\n",
    "\n",
    "\n",
    "adv_jaccard(model.predict(p_in).flatten(), target_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "918d6737",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gradients of no input are removed from consideration because they are a) small and b) mean nothing to the fuzzer for mutating inputs\n",
    "def compAbsIndexGradientMean(file_name, tar_index):\n",
    "    \n",
    "    curr_in, res = vectorize(file_name)\n",
    "\n",
    "    print(curr_in.shape)\n",
    "    #Computing the Gradient\n",
    "    analyzer = innvestigate.create_analyzer(\"integrated_gradients\", model, neuron_selection_mode=\"index\")\n",
    "\n",
    "    a = analyzer.analyze(curr_in, tar_index)\n",
    "    a = a[a != 0]\n",
    "    \n",
    "    mean = np.mean(np.absolute(a))\n",
    "    fname = file_name.split('/')[5]\n",
    "    fname = \"./targets/\"  + target_dir + \"/out/queue_grads/\" + fname + \"_\" + str(tar_index) + \".txt\"\n",
    "    with open(fname, \"w\") as file:\n",
    "       file.write(str(f\"{mean:8f}\"))\n",
    "    \n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1603ae89",
   "metadata": {},
   "outputs": [],
   "source": [
    "compAbsIndexGradientMean(seed_list[2], 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65eadf18",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Computes average gradient of input\n",
    "def compAbsGradientMean(file_name):\n",
    "    \n",
    "    curr_in, res = vectorize(file_name)\n",
    "\n",
    "    print(curr_in.shape)\n",
    "    #Computing the Gradient\n",
    "    analyzer = innvestigate.create_analyzer(\"integrated_gradients\", model)\n",
    "\n",
    "    a = analyzer.analyze(curr_in)\n",
    "    a = a[a != 0]\n",
    "    \n",
    "    mean = np.mean(np.absolute(a))\n",
    "    fname = file_name.split('/')[5]\n",
    "    fname = \"./targets/\"  + target_dir + \"/out/queue_grads/\" + fname + \".txt\"\n",
    "    with open(fname, \"w\") as file:\n",
    "       file.write(str(f\"{mean:8f}\"))\n",
    "    \n",
    "    return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2c6843",
   "metadata": {},
   "outputs": [],
   "source": [
    "compAbsGradientMean(seed_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c681fb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77203380",
   "metadata": {},
   "outputs": [],
   "source": [
    "def connectToAfl():\n",
    "        \"\"\" TCP Socket \"\"\"\n",
    "    client = socket.socket(socket.AF_INET, socket.SOCK_STREAM)\n",
    "    client.connect(ADDR)\n",
    "    print(\"Connection Success\")\n",
    "\n",
    "    \"\"\" Recv data \"\"\"\n",
    "    data = client.recv(SIZE).decode(FORMAT)\n",
    "    print(f\"[SERVER] {data}\")\n",
    "\n",
    "    \"\"\" Send data \"\"\"\n",
    "    data += \" From CLIENT\"\n",
    "    client.send(data.encode(FORMAT))\n",
    "\n",
    "    \"\"\" Close connection \"\"\"\n",
    "    client.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "cffe25aa0c3ceb0fa38d974df72065d6f31c551ca9f5fa1ad0622df966cb67c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}